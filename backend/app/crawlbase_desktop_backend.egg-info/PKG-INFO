Metadata-Version: 2.4
Name: crawlbase-desktop-backend
Version: 0.1.0
Summary: Backend FastAPI para la suite de escritorio multi-perfil de Crawlbase
Author-email: Equipo Crawlbase Desktop <dev@example.com>
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.110
Requires-Dist: uvicorn[standard]>=0.30
Requires-Dist: sqlmodel>=0.0.16
Requires-Dist: alembic>=1.13
Requires-Dist: httpx>=0.26
Requires-Dist: pydantic-settings>=2.3
Requires-Dist: python-multipart>=0.0.9
Requires-Dist: cryptography>=42.0
Requires-Dist: passlib[bcrypt]>=1.7.4
Requires-Dist: typer[all]>=0.12
Requires-Dist: rich>=13.7
Requires-Dist: celery>=5.4
Requires-Dist: redis>=5.0
Requires-Dist: openpyxl>=3.1
Requires-Dist: pandas>=2.2
Requires-Dist: python-slugify>=8.0
Requires-Dist: python-dotenv>=1.0
Requires-Dist: orjson>=3.10
Provides-Extra: dev
Requires-Dist: pytest>=8.2; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23; extra == "dev"
Requires-Dist: pytest-cov>=4.1; extra == "dev"
Requires-Dist: mypy>=1.10; extra == "dev"
Requires-Dist: ruff>=0.5; extra == "dev"
Requires-Dist: black>=24.4; extra == "dev"
Requires-Dist: isort>=5.13; extra == "dev"

# Backend – Crawlbase Desktop Suite

Este backend expone todos los servicios de dominio que alimentan la aplicación de escritorio profesional para Crawlbase. Está construido con FastAPI y se encarga de orquestar perfiles multi-suscripción, proyectos y la ejecución de scrapers especializados.

## Características principales
- API REST y WebSocket para la Shell de escritorio (Tauri/React).
- Gestión cifrada de perfiles y tokens (multi-credencial).
- Integración con todos los productos de Crawlbase (Crawling API, Smart Proxy, Crawler, Cloud Storage, etc.).
- Generador de enlaces por vertical (Amazon, Walmart, Facebook, etc.).
- Motor de documentación interactiva con ejemplos ejecutables.
- Dashboard de consumo alimentado por métricas en tiempo real.

## Requisitos
- Python 3.11+
- Redis (para tareas asíncronas con Celery)
- Node 18+ y Rust (si se compila la Shell Tauri)

## Instalación rápida
```bash
python -m venv .venv
.\.venv\Scripts\activate
pip install -U pip
pip install -e .
alembic upgrade head
python -m app.cli perfiles seed
uvicorn app.main:app --reload
```

## Estructura relevante
```
app/
  core/          # Configuración, seguridad, DB
  profiles/      # Modelos y servicios de perfiles
  projects/      # Gestión de proyectos y plantillas
  link_factory/  # Generación de listas de URLs
  crawlbase/     # Clientes y endpoints hacia Crawlbase
  docs/          # Documentación interactiva
  analytics/     # Métricas y dashboard
  tasks/         # Jobs asíncronos con Celery
tests/           # Pytest
seed_data/       # Datos iniciales (incluye primer perfil)
```

## Scripts CLI
```
python -m app.cli perfiles list
python -m app.cli proyectos create "Amazon BlackFriday"
python -m app.cli docs sync
```

## Próximos pasos
- Completar implementación de servicios y endpoints.
- Añadir cobertura Pytest/mypy.
- Conectar con la Shell de escritorio Tauri.

